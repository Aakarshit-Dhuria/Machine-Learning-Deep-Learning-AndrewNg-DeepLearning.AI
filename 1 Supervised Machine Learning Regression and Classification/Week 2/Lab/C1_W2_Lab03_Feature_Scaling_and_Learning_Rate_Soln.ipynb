{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYvWykuMz348"
      },
      "source": [
        "# Optional Lab: Feature scaling and Learning Rate (Multi-variable)"
      ],
      "id": "oYvWykuMz348"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlYoVjzWz35B"
      },
      "source": [
        "## Goals\n",
        "In this lab you will:\n",
        "- Utilize  the multiple variables routines developed in the previous lab\n",
        "- run Gradient Descent on a data set with multiple features\n",
        "- explore the impact of the *learning rate alpha* on gradient descent\n",
        "- improve performance of gradient descent by *feature scaling* using z-score normalization"
      ],
      "id": "tlYoVjzWz35B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj9KG6TDz35C"
      },
      "source": [
        "## Tools\n",
        "You will utilize the functions developed in the last lab as well as matplotlib and NumPy. "
      ],
      "id": "Aj9KG6TDz35C"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "lab_utils_common.py\n",
        "    functions common to all optional labs, Course 1, Week 2 \n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.style.use('./deeplearning.mplstyle')\n",
        "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0';\n",
        "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
        "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')\n",
        "\n",
        "\n",
        "##########################################################\n",
        "# Regression Routines\n",
        "##########################################################\n",
        "\n",
        "#Function to calculate the cost\n",
        "def compute_cost_matrix(X, y, w, b, verbose=False):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "     Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      verbose : (Boolean) If true, print out intermediate value f_wb\n",
        "    Returns\n",
        "      cost: (scalar)\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # calculate f_wb for all examples.\n",
        "    f_wb = X @ w + b\n",
        "    # calculate cost\n",
        "    total_cost = (1/(2*m)) * np.sum((f_wb-y)**2)\n",
        "\n",
        "    if verbose: print(\"f_wb:\")\n",
        "    if verbose: print(f_wb)\n",
        "\n",
        "    return total_cost\n",
        "\n",
        "def compute_gradient_matrix(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "    Returns\n",
        "      dj_dw (ndarray (n,1)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):        The gradient of the cost w.r.t. the parameter b.\n",
        "\n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    f_wb = X @ w + b\n",
        "    e   = f_wb - y\n",
        "    dj_dw  = (1/m) * (X.T @ e)\n",
        "    dj_db  = (1/m) * np.sum(e)\n",
        "\n",
        "    return dj_db,dj_dw\n",
        "\n",
        "\n",
        "# Loop version of multi-variable compute_cost\n",
        "def compute_cost(X, y, w, b):\n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "    Returns\n",
        "      cost (scalar)    : cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i],w) + b           #(n,)(n,)=scalar\n",
        "        cost = cost + (f_wb_i - y[i])**2\n",
        "    cost = cost/(2*m)\n",
        "    return cost \n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "    Returns\n",
        "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):             The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i,j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw/m\n",
        "    dj_db = dj_db/m\n",
        "\n",
        "    return dj_db,dj_dw\n"
      ],
      "metadata": {
        "id": "DKK-fjPr0me6"
      },
      "id": "DKK-fjPr0me6",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'; \n",
        "# plt.style.use('./deeplearning.mplstyle')\n",
        "\n",
        "def load_data_multi():\n",
        "    data = np.loadtxt(\"data/ex1data2.txt\", delimiter=',')\n",
        "    X = data[:,:2]\n",
        "    y = data[:,2]\n",
        "    return X, y\n",
        "\n",
        "##########################################################\n",
        "# Plotting Routines\n",
        "##########################################################\n",
        "\n",
        "def plt_house_x(X, y,f_wb=None, ax=None):\n",
        "    ''' plot house with aXis '''\n",
        "    if not ax:\n",
        "        fig, ax = plt.subplots(1,1)\n",
        "    ax.scatter(X, y, marker='x', c='r', label=\"Actual Value\")\n",
        "\n",
        "    ax.set_title(\"Housing Prices\")\n",
        "    ax.set_ylabel('Price (in 1000s of dollars)')\n",
        "    ax.set_xlabel(f'Size (1000 sqft)')\n",
        "    if f_wb is not None:\n",
        "        ax.plot(X, f_wb,  c=dlblue, label=\"Our Prediction\")\n",
        "    ax.legend()\n",
        "    \n",
        "\n",
        "def mk_cost_lines(x,y,w,b, ax):\n",
        "    ''' makes vertical cost lines'''\n",
        "    cstr = \"cost = (1/2m)*1000*(\"\n",
        "    ctot = 0\n",
        "    label = 'cost for point'\n",
        "    for p in zip(x,y):\n",
        "        f_wb_p = w*p[0]+b\n",
        "        c_p = ((f_wb_p - p[1])**2)/2\n",
        "        c_p_txt = c_p/1000\n",
        "        ax.vlines(p[0], p[1],f_wb_p, lw=3, color=dlpurple, ls='dotted', label=label)\n",
        "        label='' #just one\n",
        "        cxy = [p[0], p[1] + (f_wb_p-p[1])/2]\n",
        "        ax.annotate(f'{c_p_txt:0.0f}', xy=cxy, xycoords='data',color=dlpurple, \n",
        "            xytext=(5, 0), textcoords='offset points')\n",
        "        cstr += f\"{c_p_txt:0.0f} +\"\n",
        "        ctot += c_p\n",
        "    ctot = ctot/(len(x))\n",
        "    cstr = cstr[:-1] + f\") = {ctot:0.0f}\"\n",
        "    ax.text(0.15,0.02,cstr, transform=ax.transAxes, color=dlpurple)\n",
        "    \n",
        "    \n",
        "def inbounds(a,b,xlim,ylim):\n",
        "    xlow,xhigh = xlim\n",
        "    ylow,yhigh = ylim\n",
        "    ax, ay = a\n",
        "    bx, by = b\n",
        "    if (ax > xlow and ax < xhigh) and (bx > xlow and bx < xhigh) \\\n",
        "        and (ay > ylow and ay < yhigh) and (by > ylow and by < yhigh):\n",
        "        return(True)\n",
        "    else:\n",
        "        return(False)\n",
        "\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "def plt_contour_wgrad(x, y, hist, ax, w_range=[-100, 500, 5], b_range=[-500, 500, 5], \n",
        "                contours = [0.1,50,1000,5000,10000,25000,50000], \n",
        "                      resolution=5, w_final=200, b_final=100,step=10 ):\n",
        "    b0,w0 = np.meshgrid(np.arange(*b_range),np.arange(*w_range))\n",
        "    z=np.zeros_like(b0)\n",
        "    n,_ = w0.shape\n",
        "    for i in range(w0.shape[0]):\n",
        "        for j in range(w0.shape[1]):\n",
        "            z[i][j] = compute_cost(x, y, w0[i][j], b0[i][j] )\n",
        "   \n",
        "    CS = ax.contour(w0, b0, z, contours, linewidths=2,\n",
        "                   colors=[dlblue, dlorange, dldarkred, dlmagenta, dlpurple]) \n",
        "    ax.clabel(CS, inline=1, fmt='%1.0f', fontsize=10)\n",
        "    ax.set_xlabel(\"w\");  ax.set_ylabel(\"b\")\n",
        "    ax.set_title('Contour plot of cost J(w,b), vs b,w with path of gradient descent')\n",
        "    w = w_final; b=b_final\n",
        "    ax.hlines(b, ax.get_xlim()[0],w, lw=2, color=dlpurple, ls='dotted')\n",
        "    ax.vlines(w, ax.get_ylim()[0],b, lw=2, color=dlpurple, ls='dotted')\n",
        "\n",
        "    base = hist[0]\n",
        "    for point in hist[0::step]:\n",
        "        edist = np.sqrt((base[0] - point[0])**2 + (base[1] - point[1])**2)\n",
        "        if(edist > resolution or point==hist[-1]):\n",
        "            if inbounds(point,base, ax.get_xlim(),ax.get_ylim()):\n",
        "                plt.annotate('', xy=point, xytext=base,xycoords='data',\n",
        "                         arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 3},\n",
        "                         va='center', ha='center')\n",
        "            base=point\n",
        "    return\n",
        "\n",
        "\n",
        "# plots p1 vs p2. Prange is an array of entries [min, max, steps]. In feature scaling lab.\n",
        "def plt_contour_multi(x, y, w, b, ax, prange, p1, p2, title=\"\", xlabel=\"\", ylabel=\"\"): \n",
        "    contours = [1e2, 2e2,3e2,4e2, 5e2, 6e2, 7e2,8e2,1e3, 1.25e3,1.5e3, 1e4, 1e5, 1e6, 1e7]\n",
        "    px,py = np.meshgrid(np.linspace(*(prange[p1])),np.linspace(*(prange[p2])))\n",
        "    z=np.zeros_like(px)\n",
        "    n,_ = px.shape\n",
        "    for i in range(px.shape[0]):\n",
        "        for j in range(px.shape[1]):\n",
        "            w_ij = w\n",
        "            b_ij = b\n",
        "            if p1 <= 3: w_ij[p1] = px[i,j]\n",
        "            if p1 == 4: b_ij = px[i,j]\n",
        "            if p2 <= 3: w_ij[p2] = py[i,j]\n",
        "            if p2 == 4: b_ij = py[i,j]\n",
        "                \n",
        "            z[i][j] = compute_cost(x, y, w_ij, b_ij )\n",
        "    CS = ax.contour(px, py, z, contours, linewidths=2,\n",
        "                   colors=[dlblue, dlorange, dldarkred, dlmagenta, dlpurple]) \n",
        "    ax.clabel(CS, inline=1, fmt='%1.2e', fontsize=10)\n",
        "    ax.set_xlabel(xlabel);  ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "\n",
        "\n",
        "def plt_equal_scale(X_train, X_norm, y_train):\n",
        "    fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
        "    prange = [\n",
        "              [ 0.238-0.045, 0.238+0.045,  50],\n",
        "              [-25.77326319-0.045, -25.77326319+0.045, 50],\n",
        "              [-50000, 0,      50],\n",
        "              [-1500,  0,      50],\n",
        "              [0, 200000, 50]]\n",
        "    w_best = np.array([0.23844318, -25.77326319, -58.11084634,  -1.57727192])\n",
        "    b_best = 235\n",
        "    plt_contour_multi(X_train, y_train, w_best, b_best, ax[0], prange, 0, 1, \n",
        "                      title='Unnormalized, J(w,b), vs w[0],w[1]',\n",
        "                      xlabel= \"w[0] (size(sqft))\", ylabel=\"w[1] (# bedrooms)\")\n",
        "    #\n",
        "    w_best = np.array([111.1972, -16.75480051, -28.51530411, -37.17305735])\n",
        "    b_best = 376.949151515151\n",
        "    prange = [[ 111-50, 111+50,   75],\n",
        "              [-16.75-50,-16.75+50, 75],\n",
        "              [-28.5-8, -28.5+8,  50],\n",
        "              [-37.1-16,-37.1+16, 50],\n",
        "              [376-150, 376+150, 50]]\n",
        "    plt_contour_multi(X_norm, y_train, w_best, b_best, ax[1], prange, 0, 1, \n",
        "                      title='Normalized, J(w,b), vs w[0],w[1]',\n",
        "                      xlabel= \"w[0] (normalized size(sqft))\", ylabel=\"w[1] (normalized # bedrooms)\")\n",
        "    fig.suptitle(\"Cost contour with equal scale\", fontsize=18)\n",
        "    #plt.tight_layout(rect=(0,0,1.05,1.05))\n",
        "    fig.tight_layout(rect=(0,0,1,0.95))\n",
        "    plt.show()\n",
        "    \n",
        "def plt_divergence(p_hist, J_hist, x_train,y_train):\n",
        "\n",
        "    x=np.zeros(len(p_hist))\n",
        "    y=np.zeros(len(p_hist))\n",
        "    v=np.zeros(len(p_hist))\n",
        "    for i in range(len(p_hist)):\n",
        "        x[i] = p_hist[i][0]\n",
        "        y[i] = p_hist[i][1]\n",
        "        v[i] = J_hist[i]\n",
        "\n",
        "    fig = plt.figure(figsize=(12,5))\n",
        "    plt.subplots_adjust( wspace=0 )\n",
        "    gs = fig.add_gridspec(1, 5)\n",
        "    fig.suptitle(f\"Cost escalates when learning rate is too large\")\n",
        "    #===============\n",
        "    #  First subplot\n",
        "    #===============\n",
        "    ax = fig.add_subplot(gs[:2], )\n",
        "\n",
        "    # Print w vs cost to see minimum\n",
        "    fix_b = 100\n",
        "    w_array = np.arange(-70000, 70000, 1000)\n",
        "    cost = np.zeros_like(w_array)\n",
        "\n",
        "    for i in range(len(w_array)):\n",
        "        tmp_w = w_array[i]\n",
        "        cost[i] = compute_cost(x_train, y_train, tmp_w, fix_b)\n",
        "\n",
        "    ax.plot(w_array, cost)\n",
        "    ax.plot(x,v, c=dlmagenta)\n",
        "    ax.set_title(\"Cost vs w, b set to 100\")\n",
        "    ax.set_ylabel('Cost')\n",
        "    ax.set_xlabel('w')\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(2)) \n",
        "\n",
        "    #===============\n",
        "    # Second Subplot\n",
        "    #===============\n",
        "\n",
        "    tmp_b,tmp_w = np.meshgrid(np.arange(-35000, 35000, 500),np.arange(-70000, 70000, 500))\n",
        "    z=np.zeros_like(tmp_b)\n",
        "    for i in range(tmp_w.shape[0]):\n",
        "        for j in range(tmp_w.shape[1]):\n",
        "            z[i][j] = compute_cost(x_train, y_train, tmp_w[i][j], tmp_b[i][j] )\n",
        "\n",
        "    ax = fig.add_subplot(gs[2:], projection='3d')\n",
        "    ax.plot_surface(tmp_w, tmp_b, z,  alpha=0.3, color=dlblue)\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(2)) \n",
        "    ax.yaxis.set_major_locator(MaxNLocator(2)) \n",
        "\n",
        "    ax.set_xlabel('w', fontsize=16)\n",
        "    ax.set_ylabel('b', fontsize=16)\n",
        "    ax.set_zlabel('\\ncost', fontsize=16)\n",
        "    plt.title('Cost vs (b, w)')\n",
        "    # Customize the view angle \n",
        "    ax.view_init(elev=20., azim=-65)\n",
        "    ax.plot(x, y, v,c=dlmagenta)\n",
        "    \n",
        "    return\n",
        "\n",
        "# draw derivative line\n",
        "# y = m*(x - x1) + y1\n",
        "def add_line(dj_dx, x1, y1, d, ax):\n",
        "    x = np.linspace(x1-d, x1+d,50)\n",
        "    y = dj_dx*(x - x1) + y1\n",
        "    ax.scatter(x1, y1, color=dlblue, s=50)\n",
        "    ax.plot(x, y, '--', c=dldarkred,zorder=10, linewidth = 1)\n",
        "    xoff = 30 if x1 == 200 else 10\n",
        "    ax.annotate(r\"$\\frac{\\partial J}{\\partial w}$ =%d\" % dj_dx, fontsize=14,\n",
        "                xy=(x1, y1), xycoords='data',\n",
        "            xytext=(xoff, 10), textcoords='offset points',\n",
        "            arrowprops=dict(arrowstyle=\"->\"),\n",
        "            horizontalalignment='left', verticalalignment='top')\n",
        "\n",
        "def plt_gradients(x_train,y_train, f_compute_cost, f_compute_gradient):\n",
        "    #===============\n",
        "    #  First subplot\n",
        "    #===============\n",
        "    fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "    # Print w vs cost to see minimum\n",
        "    fix_b = 100\n",
        "    w_array = np.linspace(-100, 500, 50)\n",
        "    w_array = np.linspace(0, 400, 50)\n",
        "    cost = np.zeros_like(w_array)\n",
        "\n",
        "    for i in range(len(w_array)):\n",
        "        tmp_w = w_array[i]\n",
        "        cost[i] = f_compute_cost(x_train, y_train, tmp_w, fix_b)\n",
        "    ax[0].plot(w_array, cost,linewidth=1)\n",
        "    ax[0].set_title(\"Cost vs w, with gradient; b set to 100\")\n",
        "    ax[0].set_ylabel('Cost')\n",
        "    ax[0].set_xlabel('w')\n",
        "\n",
        "    # plot lines for fixed b=100\n",
        "    for tmp_w in [100,200,300]:\n",
        "        fix_b = 100\n",
        "        dj_dw,dj_db = f_compute_gradient(x_train, y_train, tmp_w, fix_b )\n",
        "        j = f_compute_cost(x_train, y_train, tmp_w, fix_b)\n",
        "        add_line(dj_dw, tmp_w, j, 30, ax[0])\n",
        "\n",
        "    #===============\n",
        "    # Second Subplot\n",
        "    #===============\n",
        "\n",
        "    tmp_b,tmp_w = np.meshgrid(np.linspace(-200, 200, 10), np.linspace(-100, 600, 10))\n",
        "    U = np.zeros_like(tmp_w)\n",
        "    V = np.zeros_like(tmp_b)\n",
        "    for i in range(tmp_w.shape[0]):\n",
        "        for j in range(tmp_w.shape[1]):\n",
        "            U[i][j], V[i][j] = f_compute_gradient(x_train, y_train, tmp_w[i][j], tmp_b[i][j] )\n",
        "    X = tmp_w\n",
        "    Y = tmp_b\n",
        "    n=-2\n",
        "    color_array = np.sqrt(((V-n)/2)**2 + ((U-n)/2)**2)\n",
        "\n",
        "    ax[1].set_title('Gradient shown in quiver plot')\n",
        "    Q = ax[1].quiver(X, Y, U, V, color_array, units='width', )\n",
        "    qk = ax[1].quiverkey(Q, 0.9, 0.9, 2, r'$2 \\frac{m}{s}$', labelpos='E',coordinates='figure')\n",
        "    ax[1].set_xlabel(\"w\"); ax[1].set_ylabel(\"b\")\n",
        "\n",
        "def norm_plot(ax, data):\n",
        "    scale = (np.max(data) - np.min(data))*0.2\n",
        "    x = np.linspace(np.min(data)-scale,np.max(data)+scale,50)\n",
        "    _,bins, _ = ax.hist(data, x, color=\"xkcd:azure\")\n",
        "    #ax.set_ylabel(\"Count\")\n",
        "    \n",
        "    mu = np.mean(data); \n",
        "    std = np.std(data); \n",
        "    dist = norm.pdf(bins, loc=mu, scale = std)\n",
        "    \n",
        "    axr = ax.twinx()\n",
        "    axr.plot(bins,dist, color = \"orangered\", lw=2)\n",
        "    axr.set_ylim(bottom=0)\n",
        "    axr.axis('off')\n",
        "    \n",
        "def plot_cost_i_w(X,y,hist):\n",
        "    ws = np.array([ p[0] for p in hist[\"params\"]])\n",
        "    rng = max(abs(ws[:,0].min()),abs(ws[:,0].max()))\n",
        "    wr = np.linspace(-rng+0.27,rng+0.27,20)\n",
        "    cst = [compute_cost(X,y,np.array([wr[i],-32, -67, -1.46]), 221) for i in range(len(wr))]\n",
        "\n",
        "    fig,ax = plt.subplots(1,2,figsize=(12,3))\n",
        "    ax[0].plot(hist[\"iter\"], (hist[\"cost\"]));  ax[0].set_title(\"Cost vs Iteration\")\n",
        "    ax[0].set_xlabel(\"iteration\"); ax[0].set_ylabel(\"Cost\")\n",
        "    ax[1].plot(wr, cst); ax[1].set_title(\"Cost vs w[0]\")\n",
        "    ax[1].set_xlabel(\"w[0]\"); ax[1].set_ylabel(\"Cost\")\n",
        "    ax[1].plot(ws[:,0],hist[\"cost\"])\n",
        "    plt.show()\n",
        "\n",
        " \n",
        "##########################################################\n",
        "# Regression Routines\n",
        "##########################################################\n",
        "\n",
        "def compute_gradient_matrix(X, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        " \n",
        "    Args:\n",
        "      X : (array_like Shape (m,n)) variable such as house size \n",
        "      y : (array_like Shape (m,1)) actual value \n",
        "      w : (array_like Shape (n,1)) Values of parameters of the model      \n",
        "      b : (scalar )                Values of parameter of the model      \n",
        "    Returns\n",
        "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
        "                                  \n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    f_wb = X @ w + b              \n",
        "    e   = f_wb - y                \n",
        "    dj_dw  = (1/m) * (X.T @ e)    \n",
        "    dj_db  = (1/m) * np.sum(e)    \n",
        "        \n",
        "    return dj_db,dj_dw\n",
        "\n",
        "#Function to calculate the cost\n",
        "def compute_cost_matrix(X, y, w, b, verbose=False):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "     Args:\n",
        "      X : (array_like Shape (m,n)) variable such as house size \n",
        "      y : (array_like Shape (m,)) actual value \n",
        "      w : (array_like Shape (n,)) parameters of the model \n",
        "      b : (scalar               ) parameter of the model \n",
        "      verbose : (Boolean) If true, print out intermediate value f_wb\n",
        "    Returns\n",
        "      cost: (scalar)                      \n",
        "    \"\"\" \n",
        "    m,n = X.shape\n",
        "\n",
        "    # calculate f_wb for all examples.\n",
        "    f_wb = X @ w + b  \n",
        "    # calculate cost\n",
        "    total_cost = (1/(2*m)) * np.sum((f_wb-y)**2)\n",
        "\n",
        "    if verbose: print(\"f_wb:\")\n",
        "    if verbose: print(f_wb)\n",
        "        \n",
        "    return total_cost\n",
        "\n",
        "# Loop version of multi-variable compute_cost\n",
        "def compute_cost(X, y, w, b): \n",
        "    \"\"\"\n",
        "    compute cost\n",
        "    Args:\n",
        "      X : (ndarray): Shape (m,n) matrix of examples with multiple features\n",
        "      w : (ndarray): Shape (n)   parameters for prediction   \n",
        "      b : (scalar):              parameter  for prediction   \n",
        "    Returns\n",
        "      cost: (scalar)             cost\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):                                \n",
        "        f_wb_i = np.dot(X[i],w) + b       \n",
        "        cost = cost + (f_wb_i - y[i])**2              \n",
        "    cost = cost/(2*m)                                 \n",
        "    return(np.squeeze(cost)) \n",
        "\n",
        "def compute_gradient(X, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "    Args:\n",
        "      X : (ndarray Shape (m,n)) matrix of examples \n",
        "      y : (ndarray Shape (m,))  target value of each example\n",
        "      w : (ndarray Shape (n,))  parameters of the model      \n",
        "      b : (scalar)              parameter of the model      \n",
        "    Returns\n",
        "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):                             \n",
        "        err = (np.dot(X[i], w) + b) - y[i]   \n",
        "        for j in range(n):                         \n",
        "            dj_dw[j] = dj_dw[j] + err * X[i,j]    \n",
        "        dj_db = dj_db + err                        \n",
        "    dj_dw = dj_dw/m                                \n",
        "    dj_db = dj_db/m                                \n",
        "        \n",
        "    return dj_db,dj_dw\n",
        "\n",
        "#This version saves more values and is more verbose than the assigment versons\n",
        "def gradient_descent_houses(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      X : (array_like Shape (m,n)    matrix of examples \n",
        "      y : (array_like Shape (m,))    target value of each example\n",
        "      w_in : (array_like Shape (n,)) Initial values of parameters of the model\n",
        "      b_in : (scalar)                Initial value of parameter of the model\n",
        "      cost_function: function to compute cost\n",
        "      gradient_function: function to compute the gradient\n",
        "      alpha : (float) Learning rate\n",
        "      num_iters : (int) number of iterations to run gradient descent\n",
        "    Returns\n",
        "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar)                Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "    \n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "    \n",
        "    # An array to store values at each iteration primarily for graphing later\n",
        "    hist={}\n",
        "    hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"]=[]; hist[\"iter\"]=[];\n",
        "    \n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    save_interval = np.ceil(num_iters/10000) # prevent resource exhaustion for long runs\n",
        "\n",
        "    print(f\"Iteration Cost          w0       w1       w2       w3       b       djdw0    djdw1    djdw2    djdw3    djdb  \")\n",
        "    print(f\"---------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\")\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "      \n",
        "        # Save cost J,w,b at each save interval for graphing\n",
        "        if i == 0 or i % save_interval == 0:     \n",
        "            hist[\"cost\"].append(cost_function(X, y, w, b))\n",
        "            hist[\"params\"].append([w,b])\n",
        "            hist[\"grads\"].append([dj_dw,dj_db])\n",
        "            hist[\"iter\"].append(i)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            #print(f\"Iteration {i:4d}: Cost {cost_function(X, y, w, b):8.2f}   \")\n",
        "            cst = cost_function(X, y, w, b)\n",
        "            print(f\"{i:9d} {cst:0.5e} {w[0]: 0.1e} {w[1]: 0.1e} {w[2]: 0.1e} {w[3]: 0.1e} {b: 0.1e} {dj_dw[0]: 0.1e} {dj_dw[1]: 0.1e} {dj_dw[2]: 0.1e} {dj_dw[3]: 0.1e} {dj_db: 0.1e}\")\n",
        "       \n",
        "    return w, b, hist #return w,b and history for graphing\n",
        "\n",
        "def run_gradient_descent(X,y,iterations=1000, alpha = 1e-6):\n",
        "\n",
        "    m,n = X.shape\n",
        "    # initialize parameters\n",
        "    initial_w = np.zeros(n)\n",
        "    initial_b = 0\n",
        "    # run gradient descent\n",
        "    w_out, b_out, hist_out = gradient_descent_houses(X ,y, initial_w, initial_b,\n",
        "                                               compute_cost, compute_gradient_matrix, alpha, iterations)\n",
        "    print(f\"w,b found by gradient descent: w: {w_out}, b: {b_out:0.2f}\")\n",
        "    \n",
        "    return(w_out, b_out, hist_out)\n",
        "\n",
        "# compact extaction of hist data\n",
        "#x = hist[\"iter\"]\n",
        "#J  = np.array([ p    for p in hist[\"cost\"]])\n",
        "#ws = np.array([ p[0] for p in hist[\"params\"]])\n",
        "#dj_ws = np.array([ p[0] for p in hist[\"grads\"]])\n",
        "\n",
        "#bs = np.array([ p[1] for p in hist[\"params\"]]) \n",
        "\n",
        "def run_gradient_descent_feng(X,y,iterations=1000, alpha = 1e-6):\n",
        "    m,n = X.shape\n",
        "    # initialize parameters\n",
        "    initial_w = np.zeros(n)\n",
        "    initial_b = 0\n",
        "    # run gradient descent\n",
        "    w_out, b_out, hist_out = gradient_descent(X ,y, initial_w, initial_b,\n",
        "                                               compute_cost, compute_gradient_matrix, alpha, iterations)\n",
        "    print(f\"w,b found by gradient descent: w: {w_out}, b: {b_out:0.4f}\")\n",
        "    \n",
        "    return(w_out, b_out)\n",
        "\n",
        "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      X : (array_like Shape (m,n)    matrix of examples \n",
        "      y : (array_like Shape (m,))    target value of each example\n",
        "      w_in : (array_like Shape (n,)) Initial values of parameters of the model\n",
        "      b_in : (scalar)                Initial value of parameter of the model\n",
        "      cost_function: function to compute cost\n",
        "      gradient_function: function to compute the gradient\n",
        "      alpha : (float) Learning rate\n",
        "      num_iters : (int) number of iterations to run gradient descent\n",
        "    Returns\n",
        "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar)                Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "    \n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "    \n",
        "    # An array to store values at each iteration primarily for graphing later\n",
        "    hist={}\n",
        "    hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"]=[]; hist[\"iter\"]=[];\n",
        "    \n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    save_interval = np.ceil(num_iters/10000) # prevent resource exhaustion for long runs\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db,dj_dw = gradient_function(X, y, w, b)   \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "      \n",
        "        # Save cost J,w,b at each save interval for graphing\n",
        "        if i == 0 or i % save_interval == 0:     \n",
        "            hist[\"cost\"].append(cost_function(X, y, w, b))\n",
        "            hist[\"params\"].append([w,b])\n",
        "            hist[\"grads\"].append([dj_dw,dj_db])\n",
        "            hist[\"iter\"].append(i)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            #print(f\"Iteration {i:4d}: Cost {cost_function(X, y, w, b):8.2f}   \")\n",
        "            cst = cost_function(X, y, w, b)\n",
        "            print(f\"Iteration {i:9d}, Cost: {cst:0.5e}\")\n",
        "    return w, b, hist #return w,b and history for graphing\n",
        "\n",
        "def load_house_data():\n",
        "    data = np.loadtxt(\"../data/houses.txt\", delimiter=',', skiprows=1)\n",
        "    X = data[:,:4]\n",
        "    y = data[:,4]\n",
        "    return X, y\n",
        "\n",
        "def zscore_normalize_features(X,rtn_ms=False):\n",
        "    \"\"\"\n",
        "    returns z-score normalized X by column\n",
        "    Args:\n",
        "      X : (numpy array (m,n)) \n",
        "    Returns\n",
        "      X_norm: (numpy array (m,n)) input normalized by column\n",
        "    \"\"\"\n",
        "    mu     = np.mean(X,axis=0)  \n",
        "    sigma  = np.std(X,axis=0)\n",
        "    X_norm = (X - mu)/sigma      \n",
        "\n",
        "    if rtn_ms:\n",
        "        return(X_norm, mu, sigma)\n",
        "    else:\n",
        "        return(X_norm)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "02E26pA10MUa"
      },
      "id": "02E26pA10MUa",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RzdmMkRqz35D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.set_printoptions(precision=2)"
      ],
      "id": "RzdmMkRqz35D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDqW0bxZz35E"
      },
      "source": [
        "## Notation\n",
        "\n",
        "|General <br />  Notation  | Description| Python (if applicable) |\n",
        "|: ------------|: ------------------------------------------------------------||\n",
        "| $a$ | scalar, non bold                                                      ||\n",
        "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
        "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
        "| **Regression** |         |    |     |\n",
        "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
        "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
        "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
        "| m | number of training examples | `m`|\n",
        "| n | number of features in each example | `n`|\n",
        "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
        "|  $b$           |  parameter: bias                                           | `b`    |     \n",
        "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at  $\\mathbf{x}^{(i)}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
        "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$| the gradient or partial derivative of cost with respect to a parameter $w_j$ |`dj_dw[j]`| \n",
        "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$| the gradient or partial derivative of cost with respect to a parameter $b$| `dj_db`|"
      ],
      "id": "gDqW0bxZz35E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnp6BL5Gz35F"
      },
      "source": [
        "#  Problem Statement\n",
        "\n",
        "As in the previous labs, you will use the motivating example of housing price prediction. The training data set contains many examples with 4 features (size, bedrooms, floors and age) shown in the table below. Note, in this lab, the Size feature is in sqft while earlier labs utilized 1000 sqft.  This data set is larger than the previous lab.\n",
        "\n",
        "We would like to build a linear regression model using these values so we can then predict the price for other houses - say, a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old. \n",
        "\n",
        "##  Dataset: \n",
        "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
        "| ----------------| ------------------- |----------------- |--------------|----------------------- |  \n",
        "| 952             | 2                   | 1                | 65           | 271.5                  |  \n",
        "| 1244            | 3                   | 2                | 64           | 232                    |  \n",
        "| 1947            | 3                   | 2                | 17           | 509.8                  |  \n",
        "| ...             | ...                 | ...              | ...          | ...                    |\n"
      ],
      "id": "Jnp6BL5Gz35F"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1-8x1FE5z35G",
        "outputId": "1ddd906a-8499-4f7f-b8bb-a0b38f81bd90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cb1e21c9ff46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_house_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'size(sqft)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'floors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-87c040b87d14>\u001b[0m in \u001b[0;36mload_house_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_house_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/houses.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: ../data/houses.txt not found."
          ]
        }
      ],
      "source": [
        "# load the dataset\n",
        "X_train, y_train = load_house_data()\n",
        "X_features = ['size(sqft)','bedrooms','floors','age']"
      ],
      "id": "1-8x1FE5z35G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa7hGgXxz35H"
      },
      "source": [
        "Let's view the dataset and its features by plotting each feature versus price."
      ],
      "id": "Pa7hGgXxz35H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV8iangaz35H"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
        "for i in range(len(ax)):\n",
        "    ax[i].scatter(X_train[:,i],y_train)\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"Price (1000's)\")\n",
        "plt.show()"
      ],
      "id": "YV8iangaz35H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-068aCdmz35I"
      },
      "source": [
        "Plotting each feature vs. the target, price, provides some indication of which features have the strongest influence on price. Above, increasing size also increases price. Bedrooms and floors don't seem to have a strong impact on price. Newer houses have higher prices than older houses."
      ],
      "id": "-068aCdmz35I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLzOZh0Dz35I"
      },
      "source": [
        "<a name=\"toc_15456_5\"></a>\n",
        "## Gradient Descent With Multiple Variables\n",
        "Here are the equations you developed in the last lab on gradient descent for multiple variables.:\n",
        "\n",
        "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j = 0..n-1}\\newline\n",
        "&b\\ \\ := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
        "\\end{align*}$$\n",
        "\n",
        "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2}  \\\\\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3}\n",
        "\\end{align}\n",
        "$$\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
      ],
      "id": "FLzOZh0Dz35I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkaVUZ0kz35J"
      },
      "source": [
        "## Learning Rate\n",
        "<figure>\n",
        "    <img src=\"https://github.com/Aakarshit-Dhuria/Machine-Learning-Deep-Learning-AndrewNg-DeepLearning.AI/blob/main/1%20Supervised%20Machine%20Learning%20Regression%20and%20Classification/Week%202/Lab/images/C1_W2_Lab06_learningrate.PNG?raw=1\" style=\"width:1200px;\" >\n",
        "</figure>\n",
        "The lectures discussed some of the issues related to setting the learning rate $\\alpha$. The learning rate controls the size of the update to the parameters. See equation (1) above. It is shared by all the parameters.  \n",
        "\n",
        "Let's run gradient descent and try a few settings of $\\alpha$ on our data set"
      ],
      "id": "XkaVUZ0kz35J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAbNOHD2z35J"
      },
      "source": [
        "### $\\alpha$ = 9.9e-7"
      ],
      "id": "bAbNOHD2z35J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfHhN6f4z35J"
      },
      "outputs": [],
      "source": [
        "#set alpha to 9.9e-7\n",
        "_, _, hist = run_gradient_descent(X_train, y_train, 10, alpha = 9.9e-7)"
      ],
      "id": "xfHhN6f4z35J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG1iCuHuz35K"
      },
      "source": [
        "It appears the learning rate is too high.  The solution does not converge. Cost is *increasing* rather than decreasing. Let's plot the result:"
      ],
      "id": "KG1iCuHuz35K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R93mdu6z35K"
      },
      "outputs": [],
      "source": [
        "plot_cost_i_w(X_train, y_train, hist)"
      ],
      "id": "8R93mdu6z35K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f6_qasuz35K"
      },
      "source": [
        "The plot on the right shows the value of one of the parameters, $w_0$. At each iteration, it is overshooting the optimal value and as a result, cost ends up *increasing* rather than approaching the minimum. Note that this is not a completely accurate picture as there are 4 parameters being modified each pass rather than just one. This plot is only showing $w_0$ with the other parameters fixed at benign values. In this and later plots you may notice the blue and orange lines being slightly off."
      ],
      "id": "5f6_qasuz35K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqTipB4Ez35K"
      },
      "source": [
        "\n",
        "### $\\alpha$ = 9e-7\n",
        "Let's try a bit smaller value and see what happens."
      ],
      "id": "FqTipB4Ez35K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkq3abWpz35L"
      },
      "outputs": [],
      "source": [
        "#set alpha to 9e-7\n",
        "_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 9e-7)"
      ],
      "id": "Mkq3abWpz35L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPwuj4WZz35L"
      },
      "source": [
        "Cost is decreasing throughout the run showing that alpha is not too large. "
      ],
      "id": "KPwuj4WZz35L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57mW1HrRz35L"
      },
      "outputs": [],
      "source": [
        "plot_cost_i_w(X_train, y_train, hist)"
      ],
      "id": "57mW1HrRz35L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uojQWTzwz35M"
      },
      "source": [
        "On the left, you see that cost is decreasing as it should. On the right, you can see that $w_0$ is still oscillating around the minimum, but it is decreasing each iteration rather than increasing. Note above that `dj_dw[0]` changes sign with each iteration as `w[0]` jumps over the optimal value.\n",
        "This alpha value will converge. You can vary the number of iterations to see how it behaves."
      ],
      "id": "uojQWTzwz35M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSLZRhIxz35M"
      },
      "source": [
        "### $\\alpha$ = 1e-7\n",
        "Let's try a bit smaller value for $\\alpha$ and see what happens."
      ],
      "id": "mSLZRhIxz35M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiFbMzfkz35M"
      },
      "outputs": [],
      "source": [
        "#set alpha to 1e-7\n",
        "_,_,hist = run_gradient_descent(X_train, y_train, 10, alpha = 1e-7)"
      ],
      "id": "qiFbMzfkz35M"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63RJXEIz35M"
      },
      "source": [
        "Cost is decreasing throughout the run showing that $\\alpha$ is not too large. "
      ],
      "id": "a63RJXEIz35M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYbMXiz5z35N"
      },
      "outputs": [],
      "source": [
        "plot_cost_i_w(X_train,y_train,hist)"
      ],
      "id": "iYbMXiz5z35N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SadDFkg9z35N"
      },
      "source": [
        "On the left, you see that cost is decreasing as it should. On the right you can see that $w_0$ is decreasing without crossing the minimum. Note above that `dj_w0` is negative throughout the run. This solution will also converge, though not quite as quickly as the previous example."
      ],
      "id": "SadDFkg9z35N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7Z1LHsBtz35N"
      },
      "source": [
        "## Feature Scaling \n",
        "<figure>\n",
        "    <img src=\"https://github.com/Aakarshit-Dhuria/Machine-Learning-Deep-Learning-AndrewNg-DeepLearning.AI/blob/main/1%20Supervised%20Machine%20Learning%20Regression%20and%20Classification/Week%202/Lab/images/C1_W2_Lab06_featurescalingheader.PNG?raw=1\" style=\"width:1200px;\" >\n",
        "</figure>\n",
        "The lectures described the importance of rescaling the dataset so the features have a similar range.\n",
        "If you are interested in the details of why this is the case, click on the 'details' header below. If not, the section below will walk through an implementation of how to do feature scaling."
      ],
      "id": "7Z1LHsBtz35N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlvvuZR7z35N"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size='3', color='darkgreen'><b>Details</b></font>\n",
        "</summary>\n",
        "\n",
        "Let's look again at the situation with $\\alpha$ = 9e-7. This is pretty close to the maximum value we can set $\\alpha$  to without diverging. This is a short run showing the first few iterations:\n",
        "\n",
        "<figure>\n",
        "    <img src=\"https://github.com/Aakarshit-Dhuria/Machine-Learning-Deep-Learning-AndrewNg-DeepLearning.AI/blob/main/1%20Supervised%20Machine%20Learning%20Regression%20and%20Classification/Week%202/Lab/images/C1_W2_Lab06_ShortRun.PNG?raw=1\" style=\"width:1200px;\" >\n",
        "</figure>\n",
        "\n",
        "Above, while cost is being decreased, its clear that $w_0$ is making more rapid progress than the other parameters due to its much larger gradient.\n",
        "\n",
        "The graphic below shows the result of a very long run with $\\alpha$ = 9e-7. This takes several hours.\n",
        "\n",
        "<figure>\n",
        "    <img src=\"https://github.com/Aakarshit-Dhuria/Machine-Learning-Deep-Learning-AndrewNg-DeepLearning.AI/blob/main/1%20Supervised%20Machine%20Learning%20Regression%20and%20Classification/Week%202/Lab/images/C1_W2_Lab06_LongRun.PNG?raw=1\" style=\"width:1200px;\" >\n",
        "</figure>\n",
        "    \n",
        "Above, you can see cost decreased slowly after its initial reduction. Notice the difference between `w0` and `w1`,`w2`,`w3` as well as  `dj_dw0` and `dj_dw1-3`. `w0` reaches its near final value very quickly and `dj_dw0` has quickly decreased to a small value showing that `w0` is near the final value. The other parameters were reduced much more slowly.\n",
        "\n",
        "Why is this?  Is there something we can improve? See below:\n",
        "<figure>\n",
        "    <center> <img src=\"https://github.com/Aakarshit-Dhuria/Machine-Learning-Deep-Learning-AndrewNg-DeepLearning.AI/blob/main/1%20Supervised%20Machine%20Learning%20Regression%20and%20Classification/Week%202/Lab/images/C1_W2_Lab06_scale.PNG?raw=1\"   ></center>\n",
        "</figure>   \n",
        "\n",
        "The figure above shows why $w$'s are updated unevenly. \n",
        "- $\\alpha$ is shared by all parameter updates ($w$'s and $b$).\n",
        "- the common error term is multiplied by the features for the $w$'s. (not $b$).\n",
        "- the features vary significantly in magnitude making some features update much faster than others. In this case, $w_0$ is multiplied by 'size(sqft)', which is generally > 1000,  while $w_1$ is multiplied by 'number of bedrooms', which is generally 2-4. \n",
        "    \n",
        "The solution is Feature Scaling."
      ],
      "id": "WlvvuZR7z35N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnXQa3y-z35N"
      },
      "source": [
        "The lectures discussed three different techniques: \n",
        "- Feature scaling, essentially dividing each positive feature by its maximum value, or more generally, rescale each feature by both its minimum and maximum values using (x-min)/(max-min). Both ways normalizes features to the range of -1 and 1, where the former method works for positive features which is simple and serves well for the lecture's example, and the latter method works for any features.\n",
        "- Mean normalization: $x_i := \\dfrac{x_i - \\mu_i}{max - min} $ \n",
        "- Z-score normalization which we will explore below. "
      ],
      "id": "MnXQa3y-z35N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DtkNsUWz35N"
      },
      "source": [
        "\n",
        "### z-score normalization \n",
        "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "To implement z-score normalization, adjust your input values as shown in this formula:\n",
        "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ \n",
        "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
        "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        ">**Implementation Note:** When normalizing the features, it is important\n",
        "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
        "from the model, we often want to predict the prices of houses we have not\n",
        "seen before. Given a new x value (living room area and number of bed-\n",
        "rooms), we must first normalize x using the mean and standard deviation\n",
        "that we had previously computed from the training set.\n",
        "\n",
        "**Implementation**"
      ],
      "id": "0DtkNsUWz35N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Y_EzvSz35O"
      },
      "outputs": [],
      "source": [
        "def zscore_normalize_features(X):\n",
        "    \"\"\"\n",
        "    computes  X, zcore normalized by column\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray (m,n))     : input data, m examples, n features\n",
        "      \n",
        "    Returns:\n",
        "      X_norm (ndarray (m,n)): input normalized by column\n",
        "      mu (ndarray (n,))     : mean of each feature\n",
        "      sigma (ndarray (n,))  : standard deviation of each feature\n",
        "    \"\"\"\n",
        "    # find the mean of each column/feature\n",
        "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
        "    # find the standard deviation of each column/feature\n",
        "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
        "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
        "    X_norm = (X - mu) / sigma      \n",
        "\n",
        "    return (X_norm, mu, sigma)\n",
        " \n",
        "#check our work\n",
        "#from sklearn.preprocessing import scale\n",
        "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
      ],
      "id": "k5Y_EzvSz35O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHkUTC7Fz35O"
      },
      "source": [
        "Let's look at the steps involved in Z-score normalization. The plot below shows the transformation step by step."
      ],
      "id": "HHkUTC7Fz35O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17UmaLAhz35O"
      },
      "outputs": [],
      "source": [
        "mu     = np.mean(X_train,axis=0)   \n",
        "sigma  = np.std(X_train,axis=0) \n",
        "X_mean = (X_train - mu)\n",
        "X_norm = (X_train - mu)/sigma      \n",
        "\n",
        "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
        "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
        "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
        "ax[0].set_title(\"unnormalized\")\n",
        "ax[0].axis('equal')\n",
        "\n",
        "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
        "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
        "ax[1].set_title(r\"X - $\\mu$\")\n",
        "ax[1].axis('equal')\n",
        "\n",
        "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
        "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
        "ax[2].set_title(r\"Z-score normalized\")\n",
        "ax[2].axis('equal')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
        "plt.show()"
      ],
      "id": "17UmaLAhz35O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw0__kfmz35P"
      },
      "source": [
        "The plot above shows the relationship between two of the training set parameters, \"age\" and \"size(sqft)\". *These are plotted with equal scale*. \n",
        "- Left: Unnormalized: The range of values or the variance of the 'size(sqft)' feature is much larger than that of age\n",
        "- Middle: The first step removes the mean or average value from each feature. This leaves features that are centered around zero. It's difficult to see the difference for the 'age' feature, but 'size(sqft)' is clearly around zero.\n",
        "- Right: The second step divides by the variance. This leaves both features centered at zero with a similar scale."
      ],
      "id": "Nw0__kfmz35P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLIHJik3z35P"
      },
      "source": [
        "Let's normalize the data and compare it to the original data."
      ],
      "id": "JLIHJik3z35P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjUAUgCwz35P"
      },
      "outputs": [],
      "source": [
        "# normalize the original features\n",
        "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
        "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
        "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
        "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
      ],
      "id": "qjUAUgCwz35P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oU8akf3z35P"
      },
      "source": [
        "The peak to peak range of each column is reduced from a factor of thousands to a factor of 2-3 by normalization."
      ],
      "id": "_oU8akf3z35P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32-9b_oVz35P"
      },
      "outputs": [],
      "source": [
        "fig,ax=plt.subplots(1, 4, figsize=(12, 3))\n",
        "for i in range(len(ax)):\n",
        "    norm_plot(ax[i],X_train[:,i],)\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"count\");\n",
        "fig.suptitle(\"distribution of features before normalization\")\n",
        "plt.show()\n",
        "fig,ax=plt.subplots(1,4,figsize=(12,3))\n",
        "for i in range(len(ax)):\n",
        "    norm_plot(ax[i],X_norm[:,i],)\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "ax[0].set_ylabel(\"count\"); \n",
        "fig.suptitle(\"distribution of features after normalization\")\n",
        "\n",
        "plt.show()"
      ],
      "id": "32-9b_oVz35P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWYKFgxUz35P"
      },
      "source": [
        "Notice, above, the range of the normalized data (x-axis) is centered around zero and roughly +/- 2. Most importantly, the range is similar for each feature."
      ],
      "id": "IWYKFgxUz35P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPI4qJi9z35Q"
      },
      "source": [
        "Let's re-run our gradient descent algorithm with normalized data.\n",
        "Note the **vastly larger value of alpha**. This will speed up gradient descent."
      ],
      "id": "yPI4qJi9z35Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DrhzpSnz35Q"
      },
      "outputs": [],
      "source": [
        "w_norm, b_norm, hist = run_gradient_descent(X_norm, y_train, 1000, 1.0e-1, )"
      ],
      "id": "-DrhzpSnz35Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE44ZGsfz35Q"
      },
      "source": [
        "The scaled features get very accurate results **much, much faster!**. Notice the gradient of each parameter is tiny by the end of this fairly short run. A learning rate of 0.1 is a good start for regression with normalized features.\n",
        "Let's plot our predictions versus the target values. Note, the prediction is made using the normalized feature while the plot is shown using the original feature values."
      ],
      "id": "jE44ZGsfz35Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSA6WB3Cz35Q"
      },
      "outputs": [],
      "source": [
        "#predict target using normalized features\n",
        "m = X_norm.shape[0]\n",
        "yp = np.zeros(m)\n",
        "for i in range(m):\n",
        "    yp[i] = np.dot(X_norm[i], w_norm) + b_norm\n",
        "\n",
        "    # plot predictions and targets versus original features    \n",
        "fig,ax=plt.subplots(1,4,figsize=(12, 3),sharey=True)\n",
        "for i in range(len(ax)):\n",
        "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
        "    ax[i].set_xlabel(X_features[i])\n",
        "    ax[i].scatter(X_train[:,i],yp,color=dlc[\"dlorange\"], label = 'predict')\n",
        "ax[0].set_ylabel(\"Price\"); ax[0].legend();\n",
        "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
        "plt.show()"
      ],
      "id": "XSA6WB3Cz35Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePV72SWtz35Q"
      },
      "source": [
        "The results look good. A few points to note:\n",
        "- with multiple features, we can no longer have a single plot showing results versus features.\n",
        "- when generating the plot, the normalized features were used. Any predictions using the parameters learned from a normalized training set must also be normalized."
      ],
      "id": "ePV72SWtz35Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdRvzxJz35Q"
      },
      "source": [
        "**Prediction**\n",
        "The point of generating our model is to use it to predict housing prices that are not in the data set. Let's predict the price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old. Recall, that you must normalize the data with the mean and standard deviation derived when the training data was normalized. "
      ],
      "id": "gcdRvzxJz35Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGaF5Eerz35Q"
      },
      "outputs": [],
      "source": [
        "# First, normalize out example.\n",
        "x_house = np.array([1200, 3, 1, 40])\n",
        "x_house_norm = (x_house - X_mu) / X_sigma\n",
        "print(x_house_norm)\n",
        "x_house_predict = np.dot(x_house_norm, w_norm) + b_norm\n",
        "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\")"
      ],
      "id": "tGaF5Eerz35Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-gx2FN5z35R"
      },
      "source": [
        "**Cost Contours**  \n",
        "<img align=\"left\" src=\"https://github.com/Aakarshit-Dhuria/Machine-Learning-Deep-Learning-AndrewNg-DeepLearning.AI/blob/main/1%20Supervised%20Machine%20Learning%20Regression%20and%20Classification/Week%202/Lab/images/C1_W2_Lab06_contours.PNG?raw=1\"   style=\"width:240px;\" >Another way to view feature scaling is in terms of the cost contours. When feature scales do not match, the plot of cost versus parameters in a contour plot is asymmetric. \n",
        "\n",
        "In the plot below, the scale of the parameters is matched. The left plot is the cost contour plot of w[0], the square feet versus w[1], the number of bedrooms before normalizing the features. The plot is so asymmetric, the curves completing the contours are not visible. In contrast, when the features are normalized, the cost contour is much more symmetric. The result is that updates to parameters during gradient descent can make equal progress for each parameter. \n"
      ],
      "id": "e-gx2FN5z35R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ6-kM3Dz35R"
      },
      "outputs": [],
      "source": [
        "plt_equal_scale(X_train, X_norm, y_train)"
      ],
      "id": "ZQ6-kM3Dz35R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTS3gO-Rz35R"
      },
      "source": [
        "\n",
        "## Congratulations!\n",
        "In this lab you:\n",
        "- utilized the routines for linear regression with multiple features you developed in previous labs\n",
        "- explored the impact of the learning rate  $\\alpha$ on convergence \n",
        "- discovered the value of feature scaling using z-score normalization in speeding convergence"
      ],
      "id": "UTS3gO-Rz35R"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A82V25Gz35R"
      },
      "source": [
        "## Acknowledgments\n",
        "The housing data was derived from the [Ames Housing dataset](http://jse.amstat.org/v19n3/decock.pdf) compiled by Dean De Cock for use in data science education."
      ],
      "id": "1A82V25Gz35R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0lhm8e_z35R"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "x0lhm8e_z35R"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc-autonumbering": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}